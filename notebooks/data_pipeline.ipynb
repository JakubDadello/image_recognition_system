{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541b70f-7085-42b2-803c-2e90da2c5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, path_rd, extract_to, img_size=(200, 200), batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline.\n",
    "\n",
    "        Args:\n",
    "            path_rd (str): Path to the compressed (ZIP) dataset file.\n",
    "            extract_to (str): Destination directory for extracted dataset.\n",
    "            img_size (tuple): Target image dimensions (height, width).\n",
    "            batch_size (int): Size of training batches.\n",
    "        \"\"\"\n",
    "        self.path_rd = path_rd\n",
    "        self.extract_to = extract_to\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.class_names = [\n",
    "            \"crazing\", \n",
    "            \"inclusion\", \n",
    "            \"patches\", \n",
    "            \"pitted_surface\", \n",
    "            \"rolled-in_scale\", \n",
    "            \"scratches\"\n",
    "        ]\n",
    "\n",
    "    def running_engine(self):\n",
    "        \"\"\"Execute the full pipeline and return prepared datasets.\"\"\"\n",
    "        self.extract_data()\n",
    "        self.organize_flat_structure()\n",
    "        return self.create_datasets()\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"Step 1: Extract ZIP dataset if not already extracted.\"\"\"\n",
    "        if not os.path.exists(self.extract_to) or len(os.listdir(self.extract_to)) == 0:\n",
    "            print(f\"Extracting data from {self.path_rd}...\")\n",
    "            try:\n",
    "                with ZipFile(self.path_rd, \"r\") as zip_file:\n",
    "                    zip_file.extractall(self.extract_to)\n",
    "                print(\"Extraction completed successfully.\")\n",
    "            except BadZipFile:\n",
    "                raise RuntimeError(\"Dataset ZIP is corrupted.\")\n",
    "        else:\n",
    "            print(\"Data already exists on disk.\")\n",
    "\n",
    "    def organize_flat_structure(self):\n",
    "        \"\"\"\n",
    "        Step 2: Organize flat images into class-named subfolders.\n",
    "        \"\"\"\n",
    "        print(\"Organizing images into class-specific folders...\")\n",
    "\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            split_path = os.path.join(self.extract_to, split)\n",
    "\n",
    "            if not os.path.exists(split_path):\n",
    "                continue\n",
    "\n",
    "            for filename in os.listdir(split_path):\n",
    "                file_path = os.path.join(split_path, filename)\n",
    "\n",
    "                # --- Skip directories ---\n",
    "                if os.path.isdir(file_path):\n",
    "                    continue\n",
    "\n",
    "                # --- Move file into correct class folder ---\n",
    "                for class_name in self.class_names:\n",
    "                    if filename.lower().startswith(class_name.lower()):\n",
    "                        target_dir = os.path.join(split_path, class_name)\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        shutil.move(file_path, os.path.join(target_dir, filename))\n",
    "                        break\n",
    "\n",
    "    def create_datasets(self):\n",
    "        \"\"\"\n",
    "        Step 3: Create tf.data.Dataset objects from organized directories.\n",
    "        \"\"\"\n",
    "\n",
    "        train_path = os.path.join(self.extract_to, \"train\")\n",
    "        val_path = os.path.join(self.extract_to, \"valid\")\n",
    "        test_path = os.path.join(self.extract_to, \"test\")\n",
    "\n",
    "        # --- Load datasets ---\n",
    "        train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "            train_path, image_size=self.img_size, batch_size=self.batch_size,\n",
    "            label_mode=\"categorical\"\n",
    "        )\n",
    "\n",
    "        val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "            val_path, image_size=self.img_size, batch_size=self.batch_size,\n",
    "            label_mode=\"categorical\",\n",
    "            validation_split=0.5, \n",
    "            subset=\"training\",    \n",
    "            seed=123\n",
    "        )\n",
    "\n",
    "        test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "            val_path, image_size=self.img_size, batch_size=self.batch_size,\n",
    "            label_mode=\"categorical\",\n",
    "            validation_split=0.5, \n",
    "            subset=\"validation\", \n",
    "            seed=123\n",
    "        )\n",
    "\n",
    "        # --- Preprocessing layers ---\n",
    "        rescaler = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "        augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            tf.keras.layers.RandomRotation(0.2)\n",
    "        ])\n",
    "\n",
    "        # --- Apply augmentation to training data ---\n",
    "        train_data = train_data.map(\n",
    "            lambda x, y: (augmentation(x, training=True), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        # --- Apply rescaling to all datasets ---\n",
    "        train_data = train_data.map(\n",
    "            lambda x, y: (rescaler(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        val_data = val_data.map(\n",
    "            lambda x, y: (rescaler(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        test_data = test_data.map(\n",
    "            lambda x, y: (rescaler(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        # --- Prefetch for performance --- \n",
    "        return (\n",
    "            train_data.prefetch(tf.data.AUTOTUNE),\n",
    "            val_data.prefetch(tf.data.AUTOTUNE),\n",
    "            test_data.prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = DataPipeline(\"data/dataset.zip\", \"data/steel_data\")\n",
    "    train_data, val_data, test_data = pipeline.running_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
